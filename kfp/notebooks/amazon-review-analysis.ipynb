{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as components\n",
    "from kfp.components import func_to_container_op, InputPath, OutputPath\n",
    "from typing import NamedTuple\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path:InputPath,HTML_TEMPLATE:str,TEXT_HTML_TEMPLATE:str,log_folder:str)-> NamedTuple('Outputs', [('logdir',str),('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import joblib\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    import io\n",
    "    \n",
    "    \n",
    "    buffer = io.StringIO()\n",
    "    raw_reviews = pd.read_csv(data_path)\n",
    "## print shape of dataset with rows and columns and information \n",
    "    print (\"The shape of the  data is (row, column):\"+ str(raw_reviews.shape))\n",
    "    print (raw_reviews.info())\n",
    "    raw_reviews.info(buf=buffer)\n",
    "    df_info = buffer.getvalue()\n",
    "    joblib.dump(raw_reviews, log_folder + '/raw_reviews.pkl')\n",
    "    html_content = HTML_TEMPLATE % raw_reviews.head().to_html(classes='table table-striped') + TEXT_HTML_TEMPLATE.format(df_info)\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage':'inline',\n",
    "            'source': html_content,                #'<h1>Hello, World!</h1>',\n",
    "        }]\n",
    "    }\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', ['logdir' ,'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(log_folder,json.dumps(metadata))\n",
    "    \n",
    "    #return ([(log_folder),()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# tr = pd.read_csv(\"Musical_instruments_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(log_folder:str,HTML_TEMPLATE:str,TEXT_HTML_TEMPLATE:str)-> NamedTuple('Outputs', [('logdir',str),('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    #Creating a copy\n",
    "    raw_reviews = joblib.load(open(log_folder + '/raw_reviews.pkl','rb'))\n",
    "    process_reviews=raw_reviews.copy()\n",
    "    #Checking for null values\n",
    "    print(process_reviews.isnull().sum())\n",
    "    \n",
    "    before = TEXT_HTML_TEMPLATE.format(\"Statistics before filling null values: \\n\"+ str(process_reviews.isnull().sum()))\n",
    "    process_reviews['reviewText']=process_reviews['reviewText'].fillna('Missing')\n",
    "    process_reviews['reviews']=process_reviews['reviewText']+process_reviews['summary']\n",
    "    process_reviews=process_reviews.drop(['reviewText', 'summary'], axis=1)\n",
    "    \n",
    "    after = TEXT_HTML_TEMPLATE.format(\"Statistics before filling null values: \\n\"+ str(process_reviews.isnull().sum()))\n",
    "    print(process_reviews.head())\n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % process_reviews.head().to_html(classes='table table-striped') + before+after\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage':'inline',\n",
    "            'source': html_content,              \n",
    "        }]\n",
    "    }\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['logdir' ,'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(log_folder,json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_creation(log_folder:str,HTML_TEMPLATE:str,TEXT_HTML_TEMPLATE:str)-> NamedTuple('Outputs', [('logdir',str),('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    print(process_reviews['overall'].value_counts())\n",
    "    def f(row):\n",
    "    \n",
    "        '''This function returns sentiment value based on the overall ratings from the user'''\n",
    "\n",
    "        if row['overall'] == 3.0:\n",
    "            val = 'Neutral'\n",
    "        elif row['overall'] == 1.0 or row['overall'] == 2.0:\n",
    "            val = 'Negative'\n",
    "        elif row['overall'] == 4.0 or row['overall'] == 5.0:\n",
    "            val = 'Positive'\n",
    "        else:\n",
    "            val = -1\n",
    "        return val\n",
    "    process_reviews['sentiment'] = process_reviews.apply(f, axis=1)\n",
    "    print(process_reviews.head())\n",
    "    \n",
    "    senti = TEXT_HTML_TEMPLATE.format(\"Sentiments values counts: \\n\"+ str(process_reviews['sentiment'].value_counts()))\n",
    "    print(process_reviews['sentiment'].value_counts())\n",
    "    \n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % process_reviews.head().to_html(classes='table table-striped') + senti\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage':'inline',\n",
    "            'source': html_content,              \n",
    "        }]\n",
    "    }\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['logdir' ,'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(log_folder,json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_related_feature_creation(log_folder:str,HTML_TEMPLATE:str,TEXT_HTML_TEMPLATE:str)-> NamedTuple('Outputs', [('logdir',str),('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    # new data frame which has date and year\n",
    "    new = process_reviews[\"reviewTime\"].str.split(\",\", n = 1, expand = True) \n",
    "\n",
    "    # making separate date column from new data frame \n",
    "    process_reviews[\"date\"]= new[0] \n",
    "\n",
    "    # making separate year column from new data frame \n",
    "    process_reviews[\"year\"]= new[1] \n",
    "\n",
    "    process_reviews=process_reviews.drop(['reviewTime'], axis=1)\n",
    "    df = process_reviews.head().to_html(classes='table table-striped')\n",
    "    \n",
    "        # Splitting the date \n",
    "    new1 = process_reviews[\"date\"].str.split(\" \", n = 1, expand = True) \n",
    "\n",
    "    # adding month to the main dataset \n",
    "    process_reviews[\"month\"]= new1[0] \n",
    "\n",
    "    # adding day to the main dataset \n",
    "    process_reviews[\"day\"]= new1[1] \n",
    "\n",
    "    process_reviews=process_reviews.drop(['date'], axis=1)\n",
    "    process_reviews.head()\n",
    "    df += process_reviews.head().to_html(classes='table table-striped')\n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % df\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage':'inline',\n",
    "            'source': html_content,              \n",
    "        }]\n",
    "    }\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['logdir' ,'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(log_folder,json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(log_folder:str,HTML_TEMPLATE:str,TEXT_HTML_TEMPLATE:str)-> NamedTuple('Outputs', [('logdir',str),('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    # Splitting the dataset based on comma and square bracket \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    from collections import namedtuple\n",
    "    import json\n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    \n",
    "    new1 = process_reviews[\"helpful\"].str.split(\",\", n = 1, expand = True)\n",
    "    new2 = new1[0].str.split(\"[\", n = 1, expand = True)\n",
    "    new3 = new1[1].str.split(\"]\", n = 1, expand = True)\n",
    "\n",
    "    #Resetting the index\n",
    "    new2.reset_index(drop=True, inplace=True)\n",
    "    new3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Dropping empty columns due to splitting \n",
    "    new2=new2.drop([0], axis=1)\n",
    "    new3=new3.drop([1], axis=1)\n",
    "\n",
    "    #Concatenating the splitted columns\n",
    "    helpful=pd.concat([new2, new3], axis=1)\n",
    "\n",
    "    # I found few spaces in new3, so it is better to strip all the values to find the rate\n",
    "    def trim_all_columns(df):\n",
    "        \"\"\"\n",
    "        Trim whitespace from ends of each value across all series in dataframe\n",
    "        \"\"\"\n",
    "        trim_strings = lambda x: x.strip() if isinstance(x, str) else x\n",
    "        return df.applymap(trim_strings)\n",
    "\n",
    "    #Applying the function\n",
    "    helpful= trim_all_columns(helpful)\n",
    "\n",
    "    #Converting into integer types\n",
    "    helpful[0]=helpful[0].astype(str).astype(int)\n",
    "    helpful[1]=helpful[1].astype(str).astype(int)\n",
    "\n",
    "    #Dividing the two columns, we have 0 in the second columns when dvided gives error, so I'm ignoring those errors\n",
    "    try:\n",
    "        helpful['result'] = helpful[1]/helpful[0]\n",
    "    except ZeroDivisionError:\n",
    "        helpful['result']=0\n",
    "\n",
    "    #Filling the NaN values(created due to dividing) with 0\n",
    "    helpful['result'] = helpful['result'].fillna(0)\n",
    "\n",
    "    #Rounding of the results to two decimal places\n",
    "    helpful['result']=helpful['result'].round(2) \n",
    "\n",
    "    #Attaching the results to a new column of the main dataframe\n",
    "    process_reviews['helpful_rate']=helpful['result']\n",
    "\n",
    "    #dropping the helpful column from main dataframe\n",
    "    process_reviews=process_reviews.drop(['helpful'], axis=1)\n",
    "\n",
    "    df = process_reviews.head().to_html(classes='table table-striped') \n",
    "    helpful = TEXT_HTML_TEMPLATE.format(\"Helpful rate value count\")\n",
    "    helpful += TEXT_HTML_TEMPLATE.format(process_reviews['helpful_rate'].value_counts())\n",
    "    \n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % df + helpful\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage':'inline',\n",
    "            'source': html_content,              \n",
    "        }]\n",
    "    }\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['logdir' ,'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(log_folder,json.dumps(metadata))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(log_folder:str,HTML_TEMPLATE:str,TEXT_HTML_TEMPLATE:str)-> NamedTuple('Outputs', [('logdir',str),('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import re\n",
    "    import string\n",
    "    from collections import namedtuple\n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    import json\n",
    "    \n",
    "    #Removing unnecessary columns\n",
    "    process_reviews=process_reviews.drop(['reviewerName','unixReviewTime'], axis=1)\n",
    "    #Creating a copy \n",
    "    clean_reviews=process_reviews.copy()\n",
    "    def review_cleaning(text):\n",
    "        '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "        and remove words containing numbers.'''\n",
    "        text = str(text).lower()\n",
    "        text = re.sub('\\[.*?\\]', '', text)\n",
    "        text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub('<.*?>+', '', text)\n",
    "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "        text = re.sub('\\n', '', text)\n",
    "        text = re.sub('\\w*\\d\\w*', '', text)\n",
    "        return text\n",
    "    process_reviews['reviews']=process_reviews['reviews'].apply(lambda x:review_cleaning(x))\n",
    "    print(process_reviews.head())\n",
    "    stop_words= ['yourselves', 'between', 'whom', 'itself', 'is', \"she's\", 'up', 'herself', 'here', 'your', 'each', \n",
    "                 'we', 'he', 'my', \"you've\", 'having', 'in', 'both', 'for', 'themselves', 'are', 'them', 'other',\n",
    "                 'and', 'an', 'during', 'their', 'can', 'yourself', 'she', 'until', 'so', 'these', 'ours', 'above', \n",
    "                 'what', 'while', 'have', 're', 'more', 'only', \"needn't\", 'when', 'just', 'that', 'were', \"don't\", \n",
    "                 'very', 'should', 'any', 'y', 'isn', 'who',  'a', 'they', 'to', 'too', \"should've\", 'has', 'before',\n",
    "                 'into', 'yours', \"it's\", 'do', 'against', 'on',  'now', 'her', 've', 'd', 'by', 'am', 'from', \n",
    "                 'about', 'further', \"that'll\", \"you'd\", 'you', 'as', 'how', 'been', 'the', 'or', 'doing', 'such',\n",
    "                 'his', 'himself', 'ourselves',  'was', 'through', 'out', 'below', 'own', 'myself', 'theirs', \n",
    "                 'me', 'why', 'once',  'him', 'than', 'be', 'most', \"you'll\", 'same', 'some', 'with', 'few', 'it',\n",
    "                 'at', 'after', 'its', 'which', 'there','our', 'this', 'hers', 'being', 'did', 'of', 'had', 'under',\n",
    "                 'over','again', 'where', 'those', 'then', \"you're\", 'i', 'because', 'does', 'all']\n",
    "    \n",
    "    words = TEXT_HTML_TEMPLATE.format(\"stop_words \\n \"+str(stop_words))\n",
    "    process_reviews['reviews'] = process_reviews['reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "    print(process_reviews.head())\n",
    "    words += process_reviews.head().to_html()\n",
    "    words += TEXT_HTML_TEMPLATE.format(pd.DataFrame(process_reviews.groupby('sentiment')['helpful_rate'].mean()))\n",
    "    print(pd.DataFrame(process_reviews.groupby('sentiment')['helpful_rate'].mean()))\n",
    "    \n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    joblib.dump(stop_words,log_folder + '/stop_words.pkl')\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % words\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage':'inline',\n",
    "            'source': html_content,              \n",
    "        }]\n",
    "    }\n",
    "    visualization_output = namedtuple('VisualizationOutput', ['logdir' ,'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(log_folder,json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sentiment_categories_visualize_data(log_folder:str,IMAGE_HTML_TEMPLATE:str,HTML_TEMPLATE:str)-> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "   \n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import iplot\n",
    "    from collections import namedtuple\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "        \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    \n",
    "    #plot layout\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    rcParams['figure.figsize'] = 16,9\n",
    "\n",
    "    # Creating dataframe and removing 0 helpfulrate records\n",
    "    senti_help= pd.DataFrame(process_reviews, columns = ['sentiment', 'helpful_rate'])\n",
    "    senti_help = senti_help[senti_help['helpful_rate'] != 0.00] \n",
    "\n",
    "    #Plotting phase\n",
    "    ax = sns.violinplot( x=senti_help[\"sentiment\"], y=senti_help[\"helpful_rate\"])\n",
    "   # plt.title('Sentiment vs Helpfulness')\n",
    "    ax.set_xlabel('Sentiment categories')\n",
    "    ax.set_ylabel('helpful rate')\n",
    "    \n",
    "    fig = ax.get_figure()\n",
    "    tmpfile = BytesIO()\n",
    "    fig.savefig(tmpfile, format='png')\n",
    "    encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % IMAGE_HTML_TEMPLATE.format('Sentiment vs Helpfulness',encoded)\n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content                \n",
    "        }]\n",
    "    }\n",
    "\n",
    "    \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))        \n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " def sentiment_count_visualize_data(log_folder:str,IMAGE_HTML_TEMPLATE:str,HTML_TEMPLATE:str)-> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import iplot\n",
    "    from collections import namedtuple\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    \n",
    "    #plot layout\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    rcParams['figure.figsize'] = 16,9\n",
    "\n",
    "    # Creating dataframe and removing 0 helpfulrate records\n",
    "    senti_help= pd.DataFrame(process_reviews, columns = ['sentiment', 'helpful_rate'])\n",
    "    senti_help = senti_help[senti_help['helpful_rate'] != 0.00] \n",
    "    ax = process_reviews.groupby(['year','sentiment'])['sentiment'].count().unstack().plot(legend=True)\n",
    "    ax.set_title('Year and Sentiment count')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Sentiment count')\n",
    "    fig = ax.get_figure()\n",
    "    \n",
    "    #fig = plt.figure()\n",
    "    tmpfile = BytesIO()\n",
    "    fig.savefig(tmpfile, format='png')\n",
    "    encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "    html_content = HTML_TEMPLATE % IMAGE_HTML_TEMPLATE.format('Year and Sentiment count',encoded) \n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content              \n",
    "        }]\n",
    "    }\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "    #Creating a dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_count_visualize_data(log_folder:str,IMAGE_HTML_TEMPLATE:str,HTML_TEMPLATE:str)-> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import iplot\n",
    "    from collections import namedtuple\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    \n",
    "    #plot layout\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    rcParams['figure.figsize'] = 16,9\n",
    "\n",
    "    # Creating dataframe and removing 0 helpfulrate records\n",
    "    senti_help= pd.DataFrame(process_reviews, columns = ['sentiment', 'helpful_rate'])\n",
    "    senti_help = senti_help[senti_help['helpful_rate'] != 0.00] \n",
    "    day=pd.DataFrame(process_reviews.groupby('day')['reviews'].count()).reset_index()\n",
    "    day['day']=day['day'].astype('int64')\n",
    "    day.sort_values(by=['day'])\n",
    "\n",
    "    #Plotting the graph\n",
    "    ax = sns.barplot(x=\"day\", y=\"reviews\", data=day)\n",
    "    ax.set_title('Day vs Reviews count')\n",
    "    ax.set_xlabel('Day')\n",
    "    ax.set_ylabel('Reviews count')\n",
    "    fig = ax.get_figure()\n",
    "    \n",
    "#     plt.title('Day vs Reviews count')\n",
    "#     plt.xlabel('Day')\n",
    "#     plt.ylabel('Reviews count')\n",
    "#     plt.show()\n",
    "    \n",
    "#     fig = plt.figure()\n",
    "    tmpfile = BytesIO()\n",
    "    fig.savefig(tmpfile, format='png')\n",
    "    encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "    html_content = HTML_TEMPLATE % IMAGE_HTML_TEMPLATE.format('Year and Sentiment count',encoded) \n",
    "\n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content              \n",
    "        }]\n",
    "    }\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(log_folder:str)-> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    os.system('pip install textblob')\n",
    "    from textblob import TextBlob\n",
    " \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    process_reviews['polarity'] = process_reviews['reviews'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "    process_reviews['review_len'] = process_reviews['reviews'].astype(str).apply(len)\n",
    "    process_reviews['word_count'] = process_reviews['reviews'].apply(lambda x: len(str(x).split()))\n",
    "    print(process_reviews.head())\n",
    "    print(process_reviews.columns)\n",
    "    \n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    return ([log_folder]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_visualization(log_folder:str,HTML_TEMPLATE:str,IMAGE_HTML_TEMPLATE:str)-> NamedTuple('VisualizationOutput', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import json\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install -U kaleido')\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import plot\n",
    "    from scipy import interp\n",
    "    from itertools import cycle\n",
    "    import cufflinks as cf\n",
    "    from collections import namedtuple\n",
    "#     cf.go_offline()\n",
    "#     cf.set_config_file(offline=False, world_readable=True)\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    \n",
    "\n",
    "    my_data = [go.Histogram(x = process_reviews['polarity'])]\n",
    "    my_layout = go.Layout({\"title\": 'Sentiment Polarity Distribution',\n",
    "                           \"yaxis\": {\"title\":\"count\"},\n",
    "                           \"xaxis\": {\"title\":\"polarity\"},\n",
    "                           \"showlegend\": False}, yaxis=go.layout.YAxis(\n",
    "            tickmode='array',\n",
    "            automargin=True,\n",
    "        )\n",
    "        )\n",
    "\n",
    "    fig = go.Figure(data = my_data, layout = my_layout)\n",
    "    #img_bytes = fig.to_image(format=\"png\")\n",
    "    #encoding = base64.b64encode(img_bytes).decode('utf-8')\n",
    "    #polarity = IMAGE_HTML_TEMPLATE.format(encoding) #fig.to_html(full_html=True) #\"\n",
    "    polarity = fig.to_html(full_html=True)\n",
    "    \n",
    "    \n",
    "    my_data = [go.Histogram(x = process_reviews['overall'])]\n",
    "    my_layout = go.Layout({\"title\": 'Review Rating Distribution',\n",
    "                           \"yaxis\": {\"title\":\"count\"},\n",
    "                           \"xaxis\": {\"title\":\"Rating\"},\n",
    "                           \"showlegend\": False}, yaxis=go.layout.YAxis(\n",
    "            tickmode='array',\n",
    "            automargin=True,\n",
    "        )\n",
    "        )\n",
    "\n",
    "\n",
    "    fig = go.Figure(data = my_data, layout = my_layout)\n",
    "    #tmpfile = BytesIO()\n",
    "    #fig.savefig(tmpfile, format='png')\n",
    "    #img_bytes = fig.to_image(format=\"png\")\n",
    "    #encoding = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "    #rating = IMAGE_HTML_TEMPLATE.format(encoding)\n",
    "    rating = fig.to_html(full_html=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    my_data = [go.Histogram(x = process_reviews['review_len'])]\n",
    "    my_layout = go.Layout({\"title\": \"Review Text Length Distribution\",\n",
    "                           \"yaxis\": {\"title\":\"count\"},\n",
    "                           \"xaxis\": {\"title\":\"review length\"},\n",
    "                           \"showlegend\": False}, yaxis=go.layout.YAxis(\n",
    "            tickmode='array',\n",
    "            automargin=True,\n",
    "        )\n",
    "        )\n",
    "\n",
    "\n",
    "    fig = go.Figure(data = my_data, layout = my_layout)\n",
    "    #img_bytes = fig.to_image(format=\"png\")\n",
    "    #encoding = base64.b64encode(img_bytes.getvalue()).decode('utf-8')\n",
    "    #text = IMAGE_HTML_TEMPLATE.format(encoding)\n",
    "    text = fig.to_html(full_html=True) #\"data:image/png;base64,\" + encoding\n",
    "\n",
    "    \n",
    "    my_data = [go.Histogram(x = process_reviews['word_count'])]\n",
    "    my_layout = go.Layout({\"title\": \"Review Text Word Count Distribution\",\n",
    "                           \"yaxis\": {\"title\":\"count\"},\n",
    "                           \"xaxis\": {\"title\":\"word count\"},\n",
    "                           \"showlegend\": False}, yaxis=go.layout.YAxis(\n",
    "            tickmode='array',\n",
    "            automargin=True,\n",
    "        )\n",
    "        )\n",
    "\n",
    "\n",
    "    fig = go.Figure(data = my_data, layout = my_layout)\n",
    "    #img_bytes = fig.to_image(format=\"png\")\n",
    "    #encoding = base64.b64encode(img_bytes).decode()\n",
    "    #word = \"data:image/png;base64,\" + encoding\n",
    "    word = fig.to_html(full_html=True)   \n",
    "    \n",
    "    html_content = HTML_TEMPLATE % polarity+rating+text+word\n",
    "    print(html_content[:100])\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content                #'<h1>Hello, World!</h1>',\n",
    "        }]\n",
    "    }\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    return visualization_output(json.dumps(metadata))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_analysis(log_folder:str,HTML_TEMPLATE:str)-> NamedTuple('Outputs', [('logdir',str),('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    #Filtering data\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    os.system('pip install wordcloud')\n",
    "    import pandas as pd \n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import iplot\n",
    "    from collections import namedtuple\n",
    "    from collections import defaultdict,Counter\n",
    "    import base64\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    from wordcloud import WordCloud,STOPWORDS\n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    \n",
    "    \n",
    "    review_pos = process_reviews[process_reviews[\"sentiment\"]=='Positive'].dropna()\n",
    "    review_neu = process_reviews[process_reviews[\"sentiment\"]=='Neutral'].dropna()\n",
    "    review_neg = process_reviews[process_reviews[\"sentiment\"]=='Negative'].dropna()\n",
    "    \n",
    "    print(\"Positive: \",review_pos.shape)\n",
    "    print(\"Neutral: \",review_neu.shape)\n",
    "    print(\"Negative: \",review_neg.shape)\n",
    "    ## custom function for ngram generation ##\n",
    "    def generate_ngrams(text, n_gram=1):\n",
    "        token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "        ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    ## custom function for horizontal bar chart ##\n",
    "    print(\"monogram started\")\n",
    "    print(review_pos.head())\n",
    "    def horizontal_bar_chart(df, color):\n",
    "        trace = go.Bar(\n",
    "            y=df[\"word\"].values[::-1],\n",
    "            x=df[\"wordcount\"].values[::-1],\n",
    "            showlegend=False,\n",
    "            orientation = 'h',\n",
    "            marker=dict(\n",
    "                color=color,\n",
    "            ),\n",
    "        )\n",
    "        return trace\n",
    "\n",
    "    ## Get the bar chart from positive reviews ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_pos[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent):\n",
    "            freq_dict[word] += 1\n",
    "    print(freq_dict)        \n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    print(fd_sorted)\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n",
    "\n",
    "    ## Get the bar chart from neutral reviews ##\n",
    "    \n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_neu[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace1 = horizontal_bar_chart(fd_sorted.head(25), 'grey')\n",
    "\n",
    "    ## Get the bar chart from negative reviews ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_neg[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace2 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n",
    "\n",
    "    # Creating two subplots\n",
    "    fig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,\n",
    "                              subplot_titles=[\"Frequent words of positive reviews\", \"Frequent words of neutral reviews\",\n",
    "                                              \"Frequent words of negative reviews\"])\n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1, 2, 1)\n",
    "    fig.append_trace(trace2, 3, 1)\n",
    "    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "    monogram = fig.to_html(full_html=True)\n",
    "    print(\"monogram ended\")\n",
    "    \n",
    "    ### Bigrams######\n",
    "    print(\"bigram started\")\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_pos[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent,2):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n",
    "\n",
    "    ## Get the bar chart from neutral reviews ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_neu[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent,2):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace1 = horizontal_bar_chart(fd_sorted.head(25), 'grey')\n",
    "\n",
    "    ## Get the bar chart from negative reviews ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_neg[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent,2):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace2 = horizontal_bar_chart(fd_sorted.head(25), 'brown')\n",
    "\n",
    "\n",
    "\n",
    "    # Creating two subplots\n",
    "    fig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04,horizontal_spacing=0.25,\n",
    "                              subplot_titles=[\"Bigram plots of Positive reviews\", \n",
    "                                              \"Bigram plots of Neutral reviews\",\n",
    "                                              \"Bigram plots of Negative reviews\"\n",
    "                                              ])\n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1, 2, 1)\n",
    "    fig.append_trace(trace2, 3, 1)\n",
    "\n",
    "\n",
    "    fig['layout'].update(height=1000, width=800, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots\")\n",
    "    bigram = fig.to_html(full_html=True)\n",
    "    print(\"bigram ended\")\n",
    "    \n",
    "    print(\"trigram started\")\n",
    "    for sent in review_pos[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent,3):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace0 = horizontal_bar_chart(fd_sorted.head(25), 'green')\n",
    "\n",
    "    ## Get the bar chart from neutral reviews ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_neu[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent,3):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace1 = horizontal_bar_chart(fd_sorted.head(25), 'grey')\n",
    "\n",
    "    ## Get the bar chart from negative reviews ##\n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in review_neg[\"reviews\"]:\n",
    "        for word in generate_ngrams(sent,3):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    \n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace2 = horizontal_bar_chart(fd_sorted.head(25), 'red')\n",
    "\n",
    "    # Creating two subplots\n",
    "    fig = tools.make_subplots(rows=3, cols=1, vertical_spacing=0.04, horizontal_spacing=0.05,\n",
    "                              subplot_titles=[\"Tri-gram plots of Positive reviews\", \n",
    "                                              \"Tri-gram plots of Neutral reviews\",\n",
    "                                              \"Tri-gram plots of Negative reviews\"])\n",
    "    fig.append_trace(trace0, 1, 1)\n",
    "    fig.append_trace(trace1, 2, 1)\n",
    "    fig.append_trace(trace2, 3, 1)\n",
    "    fig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\n",
    "    trigram = fig.to_html(full_html=True)\n",
    "    print(\"trigram ended\")\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % monogram+bigram+trigram\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage':'inline',\n",
    "            'source': html_content                #'<h1>Hello, World!</h1>',\n",
    "        }]\n",
    "    }\n",
    "\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', ['logdir', 'mlpipeline_ui_metadata'])\n",
    "    \n",
    "    joblib.dump(review_pos, log_folder + '/review_pos.pkl')\n",
    "    joblib.dump(review_neu, log_folder + '/review_neu.pkl')\n",
    "    joblib.dump(review_neg, log_folder + '/review_neg.pkl')\n",
    "    \n",
    "    return visualization_output(log_folder,json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud_positive_reviews(log_folder:str,HTML_TEMPLATE:str,IMAGE_HTML_TEMPLATE:str)-> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    os.system('pip install wordcloud')\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import iplot\n",
    "    from collections import namedtuple\n",
    "    import base64\n",
    "    from wordcloud import WordCloud,STOPWORDS\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    import urllib\n",
    "    \n",
    "    review_pos = joblib.load(open(log_folder + '/review_pos.pkl','rb'))\n",
    "    \n",
    "    \n",
    "    text = review_pos[\"reviews\"]\n",
    "    wordcloud = WordCloud(\n",
    "        width = 300,\n",
    "        height = 200,\n",
    "        background_color = 'black',\n",
    "        stopwords = STOPWORDS).generate(str(text))\n",
    "    fig = plt.figure(\n",
    "        figsize = (20, 20),\n",
    "        facecolor = 'k',\n",
    "        edgecolor = 'k')\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    \n",
    "    tmpfile = BytesIO()\n",
    "    plt.savefig(tmpfile, format='png')\n",
    "    tmpfile.seek(0)  # rewind the data\n",
    "    string = base64.b64encode(tmpfile.read())\n",
    "    \n",
    "    positive = IMAGE_HTML_TEMPLATE.format(\"Positive Word Cloud\",urllib.parse.quote(string))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % positive \n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content                #'<h1>Hello, World!</h1>',\n",
    "        }]\n",
    "    }\n",
    "\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return visualization_output(json.dumps(metadata))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud_neutral_reviews(log_folder:str,HTML_TEMPLATE:str,IMAGE_HTML_TEMPLATE:str)-> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    os.system('pip install wordcloud')\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import iplot\n",
    "    from collections import namedtuple\n",
    "    import base64\n",
    "    from wordcloud import WordCloud,STOPWORDS\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    import urllib\n",
    "    \n",
    "    review_neu = joblib.load(open(log_folder + '/review_neu.pkl','rb'))\n",
    "    text = review_neu[\"reviews\"]\n",
    "    wordcloud = WordCloud(\n",
    "        width = 300,\n",
    "        height = 200,\n",
    "        background_color = 'black',\n",
    "        stopwords = STOPWORDS).generate(str(text))\n",
    "    fig = plt.figure(\n",
    "        figsize = (20, 20),\n",
    "        facecolor = 'k',\n",
    "        edgecolor = 'k')\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    tmpfile = BytesIO()\n",
    "    plt.savefig(tmpfile, format='png')\n",
    "    tmpfile.seek(0)  # rewind the data\n",
    "    string = base64.b64encode(tmpfile.read())\n",
    "    \n",
    "    positive = IMAGE_HTML_TEMPLATE.format(\"Neutal word Cloud \",urllib.parse.quote(string))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % positive \n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content                #'<h1>Hello, World!</h1>',\n",
    "        }]\n",
    "    }\n",
    "  \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return visualization_output(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud_negative_reviews(log_folder:str,HTML_TEMPLATE:str,IMAGE_HTML_TEMPLATE:str)-> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install plotly')\n",
    "    os.system('pip install cufflinks')\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    os.system('pip install wordcloud')\n",
    "    import pandas as pd \n",
    "    import numpy as np \n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from plotly import tools\n",
    "    import plotly.graph_objs as go\n",
    "    from plotly.offline import iplot\n",
    "    from collections import namedtuple\n",
    "    import base64\n",
    "    from wordcloud import WordCloud,STOPWORDS\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    import urllib\n",
    "    \n",
    "    review_neg = joblib.load(open(log_folder + '/review_neg.pkl','rb'))\n",
    "    stop_words =joblib.load(open(log_folder + '/stop_words.pkl','rb'))                  \n",
    "    \n",
    "    text = review_neg[\"reviews\"]\n",
    "    wordcloud = WordCloud(\n",
    "        width = 300,\n",
    "        height = 200,\n",
    "        background_color = 'black',\n",
    "        stopwords = stop_words).generate(str(text))   \n",
    "    \n",
    "    tmpfile = BytesIO()\n",
    "    plt.savefig(tmpfile, format='png')\n",
    "    tmpfile.seek(0)  # rewind the data\n",
    "    string = base64.b64encode(tmpfile.read())\n",
    "    \n",
    "    positive = IMAGE_HTML_TEMPLATE.format(\"Negative word cloud \",urllib.parse.quote(string))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    \n",
    "    html_content = HTML_TEMPLATE % positive\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content                #'<h1>Hello, World!</h1>',\n",
    "        }]\n",
    "    }\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return visualization_output(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "   \n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    # calling the label encoder function\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_encoder = LabelEncoder() \n",
    "\n",
    "    # Encode labels in column 'sentiment'. \n",
    "    process_reviews['sentiment']= label_encoder.fit_transform(process_reviews['sentiment']) \n",
    "\n",
    "    print(process_reviews['sentiment'].unique()) \n",
    "    print(process_reviews['sentiment'].value_counts())\n",
    "    \n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    \n",
    "    return ([log_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    import re\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    \n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    stop_words = joblib.load(open(log_folder + '/stop_words.pkl','rb'))\n",
    "    #Extracting 'reviews' for processing\n",
    "    review_features=process_reviews.copy()\n",
    "    review_features=review_features[['reviews']].reset_index(drop=True)\n",
    "    print(review_features.head())\n",
    "    #Performing stemming on the review dataframe\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    #splitting and adding the stemmed words except stopwords\n",
    "    corpus = []\n",
    "    for i in range(0, len(review_features)):\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review_features['reviews'][i])\n",
    "        review = review.split()\n",
    "        review = [ps.stem(word) for word in review if not word in stop_words]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)  \n",
    "        \n",
    "    joblib.dump(process_reviews, log_folder + '/process_reviews.pkl')\n",
    "    joblib.dump(review_features, log_folder + '/review_features.pkl')\n",
    "    return ([log_folder])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    review_features = joblib.load(open(log_folder + '/review_features.pkl','rb'))\n",
    "    process_reviews = joblib.load(open(log_folder + '/process_reviews.pkl','rb'))\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000,ngram_range=(2,2))\n",
    "    # TF-IDF feature matrix\n",
    "    X= tfidf_vectorizer.fit_transform(review_features['reviews'])\n",
    "    print(X.shape)\n",
    "    #Getting the target variable(encoded)\n",
    "    y=process_reviews['sentiment']\n",
    "    \n",
    "    joblib.dump(X, log_folder + '/X.pkl')\n",
    "    joblib.dump(y, log_folder + '/y.pkl')\n",
    "    return ([log_folder]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_imbalance_data(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from collections import defaultdict,Counter\n",
    "    os.system('pip install imblearn')\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    X = joblib.load(open(log_folder + '/X.pkl','rb'))\n",
    "    y = joblib.load(open(log_folder + '/y.pkl','rb'))\n",
    "    print(f'Original dataset shape : {Counter(y)}')\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "    print(f'Resampled dataset shape {Counter(y_res)}')\n",
    "    \n",
    "    joblib.dump(X_res, log_folder + '/X_res.pkl')\n",
    "    joblib.dump(y_res, log_folder + '/y_res.pkl')\n",
    "    \n",
    "    return ([log_folder])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_spliting(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from collections import defaultdict,Counter\n",
    "    #from imblearn.over_sampling import SMOTE\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    X_res = joblib.load(open(log_folder + '/X_res.pkl','rb'))\n",
    "    y_res = joblib.load(open(log_folder + '/y_res.pkl','rb'))\n",
    "    ## Divide the dataset into Train and Test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.25, random_state=0)\n",
    "    \n",
    "    joblib.dump(X_train, log_folder + '/X_train.pkl')\n",
    "    joblib.dump(X_test, log_folder + '/X_test.pkl')\n",
    "    joblib.dump(y_train, log_folder + '/y_train.pkl')\n",
    "    joblib.dump(y_test, log_folder + '/y_test.pkl')\n",
    "    \n",
    "    return ([log_folder])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from collections import defaultdict,Counter\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.naive_bayes import BernoulliNB \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    X = joblib.load(open(log_folder + '/X_res.pkl','rb'))\n",
    "    y = joblib.load(open(log_folder + '/y_res.pkl','rb'))\n",
    "    #creating the objects\n",
    "    logreg_cv = LogisticRegression(random_state=0)\n",
    "    dt_cv=DecisionTreeClassifier()\n",
    "    knn_cv=KNeighborsClassifier()\n",
    "    svc_cv=SVC()\n",
    "    nb_cv=BernoulliNB()\n",
    "    rf_cv = RandomForestClassifier()\n",
    "    cv_dict = {0: 'Logistic Regression', 1: 'Decision Tree',2:'KNN',3:'SVC',4:'Naive Bayes',5:'Random Forest'}\n",
    "    cv_models=[logreg_cv,dt_cv,knn_cv,svc_cv,nb_cv,rf_cv]\n",
    "\n",
    "\n",
    "    for i,model in enumerate(cv_models):\n",
    "        print(\"{} Test Accuracy: {}\".format(cv_dict[i],cross_val_score(model, X, y, cv=10, scoring ='accuracy').mean()))\n",
    "    \n",
    "    joblib.dump(logreg_cv, log_folder + '/logreg_cv.pkl')\n",
    "    joblib.dump(dt_cv, log_folder + '/dt_cv.pkl')\n",
    "    joblib.dump(knn_cv, log_folder + '/knn_cv.pkl')\n",
    "    joblib.dump(svc_cv, log_folder + '/svc_cv.pkl')\n",
    "    joblib.dump(nb_cv, log_folder + '/nb_cv.pkl')\n",
    "    joblib.dump(rf_cv, log_folder + '/rf_cv.pkl')\n",
    "    \n",
    "    return ([log_folder])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(log_folder:str) -> NamedTuple('Outputs', [('logdir',str)]):\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from collections import defaultdict,Counter\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    X_train = joblib.load(open(log_folder + '/X_train.pkl','rb'))\n",
    "    X_test = joblib.load(open(log_folder + '/X_test.pkl','rb'))\n",
    "    y_train = joblib.load(open(log_folder + '/y_train.pkl','rb'))\n",
    "    y_test = joblib.load(open(log_folder + '/y_test.pkl','rb'))\n",
    "    \n",
    "    param_grid = {'C': np.logspace(-4, 4, 50), 'penalty':['l1', 'l2']}\n",
    "    clf = GridSearchCV(LogisticRegression(random_state=0), param_grid,cv=5, verbose=0,n_jobs=-1)\n",
    "    best_model = clf.fit(X_train,y_train)\n",
    "    print(best_model.best_estimator_)\n",
    "    print(\"The mean accuracy of the model is:\",best_model.score(X_test,y_test))\n",
    "    logreg = LogisticRegression(C=10000.0, random_state=0)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "    \n",
    "    joblib.dump(y_pred, log_folder + '/y_pred.pkl')\n",
    "    \n",
    "    return ([log_folder]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(log_folder:str,HTML_TEMPLATE:str,IMAGE_HTML_TEMPLATE:str,TEXT_HTML_TEMPLATE:str)-> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    import pandas as pd \n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from io import BytesIO\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import base64\n",
    "    \n",
    "    y_test = joblib.load(open(log_folder + '/y_test.pkl','rb'))\n",
    "    y_pred = joblib.load(open(log_folder + '/y_pred.pkl','rb'))\n",
    "    \n",
    "    classes=['Negative','Neutral','Positive']\n",
    "    normalize=False\n",
    "    title='Confusion matrix'\n",
    "    cmap=plt.cm.Blues  \n",
    "     \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range (cm.shape[0]):\n",
    "        for j in range (cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            \n",
    "    ax = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "            \n",
    "    print(\"Classification Report:\\n\",classification_report(y_test, y_pred))\n",
    "    report  = TEXT_HTML_TEMPLATE.format(classification_report(y_test, y_pred))\n",
    "    fig = ax.get_figure()\n",
    "    tmpfile = BytesIO()\n",
    "    fig.savefig(tmpfile, format='png')\n",
    "    encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "    html_content = HTML_TEMPLATE % report+IMAGE_HTML_TEMPLATE.format(title,encoded)\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content                #'<h1>Hello, World!</h1>',\n",
    "        }]\n",
    "    }\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return visualization_output(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# cm = metrics.confusion_matrix([1,1,1,0,0,0], [1,0,1,0,1,0])\n",
    "# classes=['Negative','Neutral','Positive']\n",
    "# cmap=plt.cm.Blues\n",
    "# ax = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "# ax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_all_and_roc_auc_curve(log_folder:str,HTML_TEMPLATE:str,IMAGE_HTML_TEMPLATE:str)-> NamedTuple('Output', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    #Binarizing the target feature\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import os\n",
    "    os.system('pip install seaborn')\n",
    "    os.system('pip install matplotlib')\n",
    "    from collections import namedtuple\n",
    "    import pandas as pd \n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from itertools import cycle\n",
    "    import json\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    import matplotlib.pyplot as plt \n",
    "    from matplotlib import rcParams\n",
    "    import seaborn as sns\n",
    "    from scipy import interp\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from io import BytesIO\n",
    "    import base64\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "#     y_test = joblib.load(open(log_folder + '/y_test.pkl','rb'))\n",
    "#     y_pred = joblib.load(open(log_folder + '/y_pred.pkl','rb')\n",
    "    \n",
    "    X = joblib.load(open(log_folder + '/X_res.pkl','rb'))\n",
    "    y = joblib.load(open(log_folder + '/y_res.pkl','rb'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    y = label_binarize(y, classes=[0, 1, 2])\n",
    "    n_classes = y.shape[1]\n",
    "\n",
    "    #Train-Test split(80:20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,\n",
    "                                                        random_state=0)\n",
    "\n",
    "    #OneVsRestClassifier\n",
    "    classifier = OneVsRestClassifier(SVC(kernel='linear', probability=True,\n",
    "                                     random_state=10))\n",
    "    y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "    #Computing TPR and FPR\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    fig,ax = plt.subplots(1, figsize=(8, 6))\n",
    "    ax.plot(fpr[\"micro\"], tpr[\"micro\"],label='micro-average ROC curve (area = {0:0.2f})'''.format(roc_auc[\"micro\"]), color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    ax.plot(fpr[\"macro\"], tpr[\"macro\"],label='macro-average ROC curve (area = {0:0.2f})'''.format(roc_auc[\"macro\"]), color='navy', linestyle=':', linewidth=4)\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        ax.plot(fpr[i], tpr[i], color=color, lw=4,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=4)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver operating characteristic to multi-class')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    \n",
    "    fig = ax.get_figure()\n",
    "    tmpfile = BytesIO()\n",
    "    fig.savefig(tmpfile, format='png')\n",
    "    encoded = base64.b64encode(tmpfile.getvalue()).decode('utf-8')\n",
    "    html_content = HTML_TEMPLATE % IMAGE_HTML_TEMPLATE.format('Receiver operating characteristic to multi-class',encoded)\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs': [{\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': html_content               \n",
    "        }]\n",
    "    }\n",
    "\n",
    "   \n",
    "    visualization_output = namedtuple('VisualizationOutput', [ 'mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return visualization_output(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Amazon Products Review',\n",
    "    description='Analysis the sentiments of reviews provided by customer to amazon products.'\n",
    ")\n",
    "\n",
    "def amazon_reviews(data_path):\n",
    "    import time\n",
    "    #data_path = 'Musical_instruments_reviews.csv'\n",
    "    log_folder = '/kfp-private'\n",
    "    pvc_name = \"amazon-review-4000\"\n",
    "    HTML_TEMPLATE = '''\n",
    "    <html><head>\n",
    "        <style>\n",
    "            table {\n",
    "                border: none;\n",
    "                border-collapse: collapse;\n",
    "                border-spacing: 0;\n",
    "                font-size: 14px;\n",
    "            }\n",
    "            td,\n",
    "            th {\n",
    "                text-align: right;\n",
    "                vertical-align: middle;\n",
    "                padding: 0.5em 0.5em;\n",
    "                line-height: 1.0;\n",
    "                white-space: nowrap;\n",
    "                max-width: 100px;\n",
    "                text-overflow: ellipsis;\n",
    "                overflow: hidden;\n",
    "                border: none;\n",
    "            }\n",
    "            th {\n",
    "                font-weight: bold;\n",
    "            }\n",
    "            tbody tr:nth-child(odd) {\n",
    "                background: rgb(245, 245, 245);\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body><div> </div>\n",
    "    <div>\n",
    "    %s\n",
    "    </div></body>\n",
    "    </html>\n",
    "    '''\n",
    "    TEXT_HTML_TEMPLATE = '''\n",
    "            <div style=\"margin:10px 0;\">\n",
    "            <pre>\n",
    "            {}\n",
    "            </pre>\n",
    "            </div>\n",
    "            '''\n",
    "    IMAGE_HTML_TEMPLATE = '''\n",
    "            <div>\n",
    "              <p>{}</p>\n",
    "              <img src=\"data:image/png;base64, {}\" />\n",
    "            </div>\n",
    "            '''\n",
    "    image = \"m10913018/nltk_env:2.3.0\"\n",
    "    \n",
    "    vop = dsl.VolumeOp(\n",
    "        name=pvc_name,\n",
    "        resource_name=\"amazon-review-4000\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO\n",
    "    )\n",
    "    \n",
    "    read_data_op = func_to_container_op(\n",
    "        func = read_data,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    data_preprocessing_op = func_to_container_op(\n",
    "        func = data_preprocessing,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    feature_creation_op = func_to_container_op(\n",
    "        func = feature_creation,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    time_related_feature_creation_op = func_to_container_op(\n",
    "        func = time_related_feature_creation,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    feature_extraction_op = func_to_container_op(\n",
    "        func = feature_extraction,\n",
    "        base_image = image,\n",
    "    )\n",
    "    data_cleaning_op = func_to_container_op(\n",
    "        func = data_cleaning,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    sentiment_categories_visualize_data_op = func_to_container_op(\n",
    "        func = sentiment_categories_visualize_data,\n",
    "        base_image = image,\n",
    "    )\n",
    "    sentiment_count_visualize_data_op = func_to_container_op(\n",
    "        func = sentiment_count_visualize_data,\n",
    "        base_image = image,\n",
    "    )\n",
    "    reviews_count_visualize_data_op = func_to_container_op(\n",
    "        func = reviews_count_visualize_data,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    new_features_op = func_to_container_op(\n",
    "        func = new_features,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    polarity_visualization_op = func_to_container_op(\n",
    "        func = polarity_visualization,\n",
    "        base_image = image,\n",
    "    )\n",
    "    ngram_analysis_op = func_to_container_op(\n",
    "        func = ngram_analysis,\n",
    "        base_image = image,\n",
    "    )\n",
    "    word_cloud_positive_reviews_op = func_to_container_op(\n",
    "        func = word_cloud_positive_reviews,\n",
    "        base_image = image,\n",
    "    )\n",
    "    word_cloud_neutral_reviews_op = func_to_container_op(\n",
    "        func = word_cloud_neutral_reviews,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    word_cloud_negative_reviews_op = func_to_container_op(\n",
    "        func = word_cloud_negative_reviews,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    target_encoding_op = func_to_container_op(\n",
    "        func = target_encoding,\n",
    "        base_image = image,\n",
    "    )\n",
    "    stemming_op = func_to_container_op(\n",
    "        func = stemming,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    tfidf_op = func_to_container_op(\n",
    "        func = tfidf,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    handling_imbalance_data_op = func_to_container_op(\n",
    "        func = handling_imbalance_data,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    data_spliting_op = func_to_container_op(\n",
    "        func = data_spliting,\n",
    "        base_image = image,\n",
    "    )\n",
    "    model_selection_op = func_to_container_op(\n",
    "        func = model_selection,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    hyperparameter_tuning_op = func_to_container_op(\n",
    "        func = hyperparameter_tuning,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "    plot_metrics_op = func_to_container_op(\n",
    "        func = plot_metrics,\n",
    "        base_image = image,\n",
    "    )\n",
    "    one_vs_all_and_roc_auc_curve_op = func_to_container_op(\n",
    "        func = one_vs_all_and_roc_auc_curve,\n",
    "        base_image = image,\n",
    "    )\n",
    "    \n",
    "#     http_op = func_to_container_op(\n",
    "#         func = http_port,\n",
    "#         base_image = image,\n",
    "#     )\n",
    "    \n",
    "    read_data_task = read_data_op(data_path,HTML_TEMPLATE,TEXT_HTML_TEMPLATE,log_folder).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    data_preprocessing_task = data_preprocessing_op(read_data_task.outputs['logdir'],HTML_TEMPLATE,TEXT_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    feature_creation_task = feature_creation_op(data_preprocessing_task.outputs['logdir'],HTML_TEMPLATE,TEXT_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    time_related_feature_creation_task = time_related_feature_creation_op(feature_creation_task.outputs['logdir'],HTML_TEMPLATE,TEXT_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    feature_extraction_task = feature_extraction_op(time_related_feature_creation_task.outputs['logdir'],HTML_TEMPLATE,TEXT_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    data_cleaning_task = data_cleaning_op(feature_extraction_task.outputs['logdir'],HTML_TEMPLATE,TEXT_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    \n",
    "    sentiment_categories_visualize_data_task = sentiment_categories_visualize_data_op(data_cleaning_task.outputs['logdir'],IMAGE_HTML_TEMPLATE,HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    sentiment_count_visualize_data_task = sentiment_count_visualize_data_op(data_cleaning_task.outputs['logdir'],IMAGE_HTML_TEMPLATE,HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    reviews_count_visualize_data_task = reviews_count_visualize_data_op(data_cleaning_task.outputs['logdir'],IMAGE_HTML_TEMPLATE,HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "       \n",
    "    new_features_task = new_features_op(data_cleaning_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    polarity_visualization_task = polarity_visualization_op(new_features_task.outputs['logdir'],HTML_TEMPLATE,IMAGE_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    ngram_analysis_task = ngram_analysis_op(new_features_task.outputs['logdir'],HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    word_cloud_positive_reviews_task = word_cloud_positive_reviews_op(ngram_analysis_task.outputs['logdir'],HTML_TEMPLATE,IMAGE_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    word_cloud_neutral_reviews_task = word_cloud_neutral_reviews_op(ngram_analysis_task.outputs['logdir'],HTML_TEMPLATE,IMAGE_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    word_cloud_negative_reviews_task = word_cloud_negative_reviews_op(ngram_analysis_task.outputs['logdir'],HTML_TEMPLATE,IMAGE_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "     \n",
    "    #time.sleep(10)\n",
    "    \n",
    "    target_encoding_task = target_encoding_op(ngram_analysis_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    stemming_task = stemming_op(target_encoding_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    tfidf_task = tfidf_op(stemming_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    handling_imbalance_data_task = handling_imbalance_data_op(tfidf_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    data_spliting_task = data_spliting_op(handling_imbalance_data_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    model_selection_task = model_selection_op(data_spliting_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    hyperparameter_tuning_task = hyperparameter_tuning_op(model_selection_task.outputs['logdir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    plot_metrics_task = plot_metrics_op(hyperparameter_tuning_task.outputs['logdir'],HTML_TEMPLATE,IMAGE_HTML_TEMPLATE,TEXT_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    one_vs_all_and_roc_auc_curve_task = one_vs_all_and_roc_auc_curve_op(hyperparameter_tuning_task.outputs['logdir'],HTML_TEMPLATE,IMAGE_HTML_TEMPLATE).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# ax = sns.violinplot()\n",
    "# ax.set_xlabel(\"abcd\")\n",
    "# ax.set_title(\"dfdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfp.compiler.Compiler().compile(movie_reviews, 'cornell-1000.zip')\n",
    "from datetime import datetime\n",
    "EXPERIMENT_NAME = \"amazon-products-review\"\n",
    "kfp_endpoint =  \"http://ml-pipeline.kubeflow.svc.cluster.local:8888\" \n",
    "arguments ={'data_path':'https://raw.githubusercontent.com/katonic-dev/Examples/master/data/Musical_instruments_reviews.csv'}\n",
    "# data_path = '/kfp-project/Musical_instruments_reviews.csv'\n",
    "pipeline_func = amazon_reviews\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.yaml'\n",
    "\n",
    "\n",
    "# pipeline_file_path = '/home/katonic/kfp-project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/experiments/details/92334c63-11cf-4c96-aa34-eb76c19eecf1\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/runs/details/b7cc05d3-76c2-440c-928f-d46f3dc0e1a6\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "client = kfp.Client(kfp_endpoint)\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "#client.pipeline_uploads.upload_pipeline(uploadfile, **kwargs)\n",
    "# pipeline_file = os.path.join(pipeline_file_path,'Musical_instruments_reviews.csv')\n",
    "# print(pipeline_file)\n",
    "run_name = pipeline_func.__name__ + str(datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
    "# pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file, name=run_name)\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import ipykernel\n",
    "# os.path.basename(ipykernel.get_connection_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "katonic/run:base",
   "experiment": {
    "id": "new",
    "name": "test-exp"
   },
   "experiment_name": "test-exp",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "test-pipe",
   "pipeline_name": "test-pipe",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwo",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
