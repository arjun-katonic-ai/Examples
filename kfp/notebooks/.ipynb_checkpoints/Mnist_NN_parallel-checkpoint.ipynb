{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e22cae-5080-4b0e-96c8-1ad9083449bb",
   "metadata": {},
   "source": [
    "# This Notebook focuses on how to use dsl.ParallelFor for parallelism in kfp sdk with a tensorflow NN usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b973d-1870-4f52-9a52-77629aa4e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import func_to_container_op\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af65f2de-30ae-496f-b6a1-f1989d620d56",
   "metadata": {},
   "source": [
    "## Loading the mnist data\n",
    "\n",
    "- This method downloads the mnist dataset and saves them to log_folder path and returning data_folder with updated path\n",
    "    parameters\n",
    "    -------------\n",
    "    log_folder: str\n",
    "        the folder to save the dataset\n",
    "    \n",
    "    return\n",
    "    ---------\n",
    "    datadir: str\n",
    "        The folder path where dataset got saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e599459-ddb2-45b9-bbae-f1ba49b28e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(log_folder: str) -> NamedTuple('Outputs', [('datadir', str)]):\n",
    "\n",
    "    import os\n",
    "    os.system(\"pip install joblib\")\n",
    "    import tensorflow as tf\n",
    "    import joblib\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    if not os.path.isdir(log_folder + '/data'):\n",
    "        os.makedirs(log_folder + '/data')\n",
    "    \n",
    "    data_folder = log_folder + '/data'\n",
    "    joblib.dump(X_train_full, data_folder + '/X_train_full.pkl')\n",
    "    joblib.dump(y_train_full, data_folder + '/y_train_full.pkl')\n",
    "    joblib.dump(X_test, data_folder + '/X_test.pkl')\n",
    "    joblib.dump(y_test, data_folder + '/y_test.pkl')\n",
    "    return ([data_folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d9c13-270a-4881-942c-2185a4d27d79",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "-    This method splits the datset into train and test\n",
    "     and normalizes the values then saves the data to pkl file and returns data_folder\n",
    "     \n",
    "    parameters\n",
    "    ------------\n",
    "    data_folder: str\n",
    "        The path to the folder of the data\n",
    "    \n",
    "    returns\n",
    "    ------------\n",
    "        path to the folder of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7861ecf4-65aa-44e3-a559-f36ab7dd8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_folder: str) -> NamedTuple('Outputs', [('data_folder', str)]):\n",
    "    \"\"\" \n",
    "\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.system(\"pip install joblib\")\n",
    "    import joblib\n",
    "    X_train_full = joblib.load(open(data_folder + '/X_train_full.pkl', 'rb'))\n",
    "    y_train_full = joblib.load(open(data_folder + '/y_train_full.pkl', 'rb'))\n",
    "    X_test = joblib.load(open(data_folder + '/X_test.pkl', 'rb'))\n",
    "    X_valid, X_train = X_train_full[:5000] / 255, X_train_full[5000:] / 255\n",
    "    y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "    X_test = X_test / 255\n",
    "    \n",
    "    joblib.dump(X_train, data_folder + '/X_train.pkl')\n",
    "    joblib.dump(X_valid, data_folder + '/X_valid.pkl')\n",
    "    joblib.dump(y_train, data_folder + '/y_train.pkl')\n",
    "    joblib.dump(y_valid, data_folder + '/y_valid.pkl')\n",
    "    joblib.dump(X_test, data_folder + '/X_test.pkl')\n",
    "    \n",
    "    return ([data_folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d65c7-8a89-41ef-8461-908ae7049511",
   "metadata": {},
   "source": [
    "## Building and training the Neural network\n",
    "\n",
    "-     This method builds and trains the neural network\n",
    "      and returns the result\n",
    "    \n",
    "    parameters\n",
    "    ---------------\n",
    "    data_folder: str\n",
    "        Path to the folder of data\n",
    "    learning_rate: float\n",
    "        Learning rate on which model needs to be trained\n",
    "        \n",
    "    return\n",
    "    --------------\n",
    "    result: list\n",
    "        A list of value consists of loss value and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82b59c1d-d4f1-43a1-8d05-3ed63444b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_model(data_folder: str, learning_rate: float) -> NamedTuple('Output', [('result', list)]):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    os.system(\"pip install joblib pandas\")\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import json\n",
    "    # a method to generate unique path for logging callbacks\n",
    "    def get_log_path(data_folder: str):\n",
    "        import os\n",
    "        import time\n",
    "        log_folder = data_folder.split('/')[0]\n",
    "        unique = time.asctime().replace(\" \", \"_\").replace(\":\",\"\")\n",
    "        if not os.path.isdir(log_folder + '/fit'):\n",
    "            os.makedirs(log_folder + '/fit')\n",
    "        log_folder = log_folder + '/fit'\n",
    "        os.makedirs(log_folder + '/' + unique)\n",
    "        log_path = log_folder + '/' + unique\n",
    "        print(f\"Saving logs at: {log_path}\")\n",
    "        return log_path\n",
    "    \n",
    "    X_train = joblib.load(open(data_folder + '/X_train.pkl', 'rb'))\n",
    "    y_train = joblib.load(open(data_folder + '/y_train.pkl', 'rb'))\n",
    "    X_valid = joblib.load(open(data_folder + '/X_valid.pkl', 'rb'))\n",
    "    y_valid = joblib.load(open(data_folder + '/y_valid.pkl', 'rb'))\n",
    "    X_test = joblib.load(open(data_folder + '/X_test.pkl', 'rb'))\n",
    "    y_test = joblib.load(open(data_folder + '/y_test.pkl', 'rb'))\n",
    "    LAYERS = [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28), name='inputLayer'),\n",
    "            tf.keras.layers.Dense(300, activation='relu', name='hiddenLayer1'),\n",
    "            tf.keras.layers.Dense(100, activation='relu', name='hiddenLayer2'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax', name='outputLayer')\n",
    "            ]\n",
    "    model_clf = tf.keras.models.Sequential(LAYERS)\n",
    "    loss_function = 'sparse_categorical_crossentropy'\n",
    "    optimizers = tf.keras.optimizers.SGD(learning_rate=learning_rate,name='SGD')\n",
    "    metric = ['accuracy']\n",
    "    model_clf.compile(loss=loss_function, \n",
    "                      optimizer=optimizers, \n",
    "                      metrics=metric)\n",
    "    log_dir = get_log_path(data_folder)\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    earlystopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ckpt_path = data_folder + \"/model_ckpt.h5\"\n",
    "    ckpt_cb = tf.keras.callbacks.ModelCheckpoint(ckpt_path, save_best_only=True)\n",
    "    CALLBACKS_LIST = [tensorboard_cb, earlystopping_cb, ckpt_cb]\n",
    "    EPOCHS = 20\n",
    "    VALIDATION = (X_valid, y_valid)\n",
    "\n",
    "    history = model_clf.fit(X_train, y_train,\n",
    "                  epochs=EPOCHS, \n",
    "                  validation_data=VALIDATION,\n",
    "                  callbacks=CALLBACKS_LIST)\n",
    "    \n",
    "    result = model_clf.evaluate(X_test, y_test)\n",
    " \n",
    "    return ([result])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4b56d-65ff-4e82-9367-b48084818a22",
   "metadata": {},
   "source": [
    "## Print the result\n",
    "\n",
    "-   Prints the result from neural network\n",
    "    parameter\n",
    "    -----------\n",
    "    result: list\n",
    "         A list of value consists of loss value and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3910bd71-aad0-44e4-aa0a-b04911159e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_result(result):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d21bcd-6515-4204-8f48-50c4a24358c6",
   "metadata": {},
   "source": [
    "## Building the pipeline as required\n",
    "- Here pipeline is designed components are created from functions, shared volume space is attached to each components\n",
    "\n",
    "    vop is a volume storage object created using dsl.VolumeOp that can be attacked to different components\n",
    "    \n",
    "    func_to_container_op is a function that is used to convert functions to components\n",
    "    \n",
    "    add_pvolumes is used to attached the shared volume vop to the components\n",
    "    \n",
    "    dsl.ParallelFor is used to run training component parallely with different learning rates, where parallelism controls \n",
    "    maximum concurrent components to run at a time.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3310b17-69e0-4717-b083-850aec8f88c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "@dsl.pipeline(\n",
    "   name='Mnist pipeline',\n",
    "   description='A pipeline that trains on mnist dataset'\n",
    ")\n",
    "def mnist_pipeline():\n",
    "    log_folder = '/information'\n",
    "    pvc_name = \"mnist-model\"\n",
    "\n",
    "    image = 'tensorflow/tensorflow'\n",
    "    \n",
    "    # Creating volume\n",
    "    vop = dsl.VolumeOp(\n",
    "        name=pvc_name,\n",
    "        resource_name=\"mnist-model\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO\n",
    "    )\n",
    "\n",
    "    # create componets from functions\n",
    "    load_data_op = func_to_container_op(\n",
    "    load_data, base_image=image)\n",
    "    \n",
    "    split_data_op = func_to_container_op(\n",
    "    split_data, base_image=image)\n",
    "    \n",
    "    training_op = func_to_container_op(\n",
    "    create_train_model, \n",
    "        base_image=image)\n",
    "    print_result_op = func_to_container_op(\n",
    "    print_result, \n",
    "        )\n",
    "\n",
    "    load_task = load_data_op(log_folder).add_pvolumes({ log_folder:vop.volume, })\n",
    "    split_data_task =split_data_op(load_task.outputs['datadir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    \n",
    "    loop_args = [0.001, 0.01, 0.1, 1] # different values of learing rate\n",
    "    with dsl.ParallelFor(loop_args, parallelism=10) as item: # parallelism: this controls the number of pod to run parallely\n",
    "        train_model_task  = training_op(split_data_task.outputs['data_folder'], item).add_pvolumes({ log_folder:vop.volume, })\n",
    "        print_result_task  = print_result_op(train_model_task.outputs['result']).add_pvolumes({ log_folder:vop.volume, })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb376b-e474-40ba-ab0d-5c5c141f66a5",
   "metadata": {},
   "source": [
    "## Run the pipeline\n",
    "- Here we create experiments, compile and finally run the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5e94eca-a865-4222-aee6-6ff8d160f2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/experiments/details/93f3754e-02a5-490e-8ad8-f80c23fcad72\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://ml-pipeline.kubeflow.svc.cluster.local:8888/#/runs/details/26d565fe-6fee-4b55-967a-e8860524787a\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "EXPERIMENT_NAME = 'Mnist'\n",
    "run_name = 'mnist_training'\n",
    "arguments ={}\n",
    "pipeline_func = mnist_pipeline\n",
    "# pipeline config filename\n",
    "pipeline_filename = pipeline_func.__name__ + f'{uuid.uuid1()}.pipeline.yaml'\n",
    "from datetime import datetime\n",
    "# compiling the pipeline and generating config filename\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "run_name = pipeline_func.__name__ + str(datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
    "client.upload_pipeline(pipeline_filename)\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946e105-849e-402f-a968-f36e019c52dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
