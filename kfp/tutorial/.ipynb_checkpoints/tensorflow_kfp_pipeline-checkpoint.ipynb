{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c5a70d-88e3-4616-a62f-729f6422e8d8",
   "metadata": {},
   "source": [
    "# This Notebook focuses on tensorflow NN usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8462fded-65b3-4362-bc0f-9df6af43ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "from kfp.components import create_component_from_func\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17481d52-c9fe-4412-9d4d-38c8c498b8c1",
   "metadata": {},
   "source": [
    "## Loading the mnist data\n",
    "\n",
    "- This method downloads the mnist dataset and saves them to log_folder path and returning data_folder with updated path\n",
    "    parameters\n",
    "    -------------\n",
    "    log_folder: str\n",
    "        the folder to save the dataset\n",
    "    \n",
    "    return\n",
    "    ---------\n",
    "    datadir: str\n",
    "        The folder path where dataset got saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7d2036-0d3a-4083-a635-687f36042a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(log_folder: str) -> NamedTuple('Outputs', [('datadir', str)]):\n",
    "    \"\"\"\n",
    "    This method downloads the mnist dataset and saves them to log_folder path\n",
    "    \n",
    "    parameters\n",
    "    -------------\n",
    "    log_folder: str\n",
    "        the folder to save the dataset\n",
    "    \n",
    "    return\n",
    "    ---------\n",
    "    datadir: str\n",
    "        The folder path where dataset got saved\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.system(\"pip install joblib\")\n",
    "    import tensorflow as tf\n",
    "    import joblib\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    if not os.path.isdir(log_folder + '/data'):\n",
    "        os.makedirs(log_folder + '/data')\n",
    "    \n",
    "    data_folder = log_folder + '/data'\n",
    "    joblib.dump(X_train_full, data_folder + '/X_train_full.pkl')\n",
    "    joblib.dump(y_train_full, data_folder + '/y_train_full.pkl')\n",
    "    joblib.dump(X_test, data_folder + '/X_test.pkl')\n",
    "    joblib.dump(y_test, data_folder + '/y_test.pkl')\n",
    "    return ([data_folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad17047-c9c0-4bac-8962-d19ed029545c",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "\n",
    "-    This method splits the datset into train and test\n",
    "     and normalizes the values then saves the data to pkl file and returns data_folder\n",
    "     \n",
    "    \n",
    "    parameters\n",
    "    ------------\n",
    "    data_folder: str\n",
    "        The path to the folder of the data\n",
    "    \n",
    "    returns\n",
    "    ------------\n",
    "        path to the folder of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f2c09a-e8ab-4058-bf7e-6f9e2cd28baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_folder: str) -> NamedTuple('Outputs', [('data_folder', str)]):\n",
    "    \"\"\"\n",
    "    This method splits the datset into train and test\n",
    "    and normalizes the values\n",
    "    \n",
    "    parameters\n",
    "    ------------\n",
    "    data_folder: str\n",
    "        The path to the folder of the data\n",
    "    \n",
    "    returns\n",
    "    ------------\n",
    "        path to the folder of the data\n",
    "    \"\"\"\n",
    "    import os\n",
    "    os.system(\"pip install joblib\")\n",
    "    import joblib\n",
    "    X_train_full = joblib.load(open(data_folder + '/X_train_full.pkl', 'rb'))\n",
    "    y_train_full = joblib.load(open(data_folder + '/y_train_full.pkl', 'rb'))\n",
    "    X_test = joblib.load(open(data_folder + '/X_test.pkl', 'rb'))\n",
    "    X_valid, X_train = X_train_full[:5000] / 255, X_train_full[5000:] / 255\n",
    "    y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "\n",
    "    X_test = X_test / 255\n",
    "    \n",
    "    joblib.dump(X_train, data_folder + '/X_train.pkl')\n",
    "    joblib.dump(X_valid, data_folder + '/X_valid.pkl')\n",
    "    joblib.dump(y_train, data_folder + '/y_train.pkl')\n",
    "    joblib.dump(y_valid, data_folder + '/y_valid.pkl')\n",
    "    joblib.dump(X_test, data_folder + '/X_test.pkl')\n",
    "    \n",
    "    return ([data_folder])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc69e6-7e0a-435c-a51a-f39da31288df",
   "metadata": {},
   "source": [
    "## Building and training the Neural network\n",
    "\n",
    "-     This method builds and trains the neural network\n",
    "      and returns the result\n",
    "    \n",
    "    parameters\n",
    "    ---------------\n",
    "    data_folder: str\n",
    "        Path to the folder of data\n",
    "    learning_rate: float\n",
    "        Learning rate on which model needs to be trained\n",
    "        \n",
    "    return\n",
    "    --------------\n",
    "    result: list\n",
    "        A list of value consists of loss value and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "569f6543-650f-415a-9f12-de6d81496df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_model(data_folder: str):\n",
    "    \"\"\"\n",
    "    This method builds and trains the neural network\n",
    "    and returns the result\n",
    "    \n",
    "    parameters\n",
    "    ---------------\n",
    "    data_folder: str\n",
    "        Path to the folder of data\n",
    "    learning_rate: float\n",
    "        Learning rate on which model needs to be trained\n",
    "        \n",
    "    return\n",
    "    --------------\n",
    "    result: list\n",
    "        A list of value consists of loss value and accuracy\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    os.system(\"pip install joblib pandas\")\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import json\n",
    "    \n",
    "    X_train = joblib.load(open(data_folder + '/X_train.pkl', 'rb'))\n",
    "    y_train = joblib.load(open(data_folder + '/y_train.pkl', 'rb'))\n",
    "    X_valid = joblib.load(open(data_folder + '/X_valid.pkl', 'rb'))\n",
    "    y_valid = joblib.load(open(data_folder + '/y_valid.pkl', 'rb'))\n",
    "    X_test = joblib.load(open(data_folder + '/X_test.pkl', 'rb'))\n",
    "    y_test = joblib.load(open(data_folder + '/y_test.pkl', 'rb'))\n",
    "    LAYERS = [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28), name='inputLayer'),\n",
    "            tf.keras.layers.Dense(300, activation='relu', name='hiddenLayer1'),\n",
    "            tf.keras.layers.Dense(100, activation='relu', name='hiddenLayer2'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax', name='outputLayer')\n",
    "            ]\n",
    "    model_clf = tf.keras.models.Sequential(LAYERS)\n",
    "    loss_function = 'sparse_categorical_crossentropy'\n",
    "    optimizers = tf.keras.optimizers.SGD(learning_rate=0.1,name='SGD')\n",
    "    metric = ['accuracy']\n",
    "    model_clf.compile(loss=loss_function, \n",
    "                      optimizer=optimizers, \n",
    "                      metrics=metric)\n",
    "\n",
    "\n",
    "    ckpt_path = data_folder + \"/model_ckpt.h5\"\n",
    "    ckpt_cb = tf.keras.callbacks.ModelCheckpoint(ckpt_path, save_best_only=True)\n",
    "    CALLBACKS_LIST = [ckpt_cb]\n",
    "    EPOCHS = 20\n",
    "    VALIDATION = (X_valid, y_valid)\n",
    "\n",
    "    history = model_clf.fit(X_train, y_train,\n",
    "                  epochs=EPOCHS, \n",
    "                  validation_data=VALIDATION,\n",
    "                  callbacks=CALLBACKS_LIST)\n",
    "    \n",
    "    result = model_clf.evaluate(X_test, y_test)\n",
    "    print(result)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08aab88-f562-4698-a8d0-a2903bae64ca",
   "metadata": {},
   "source": [
    "## Building the pipeline as required\n",
    "- Here pipeline is designed components are created from functions, shared volume space is attached to each components\n",
    "\n",
    "    vop is a volume storage object created using dsl.VolumeOp that can be attacked to different components\n",
    "    \n",
    "    func_to_container_op is a function that is used to convert functions to components\n",
    "    \n",
    "    add_pvolumes is used to attached the shared volume vop to the components\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d340df-53a1-43d6-9510-fad09736e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "@dsl.pipeline(\n",
    "   name='Mnist pipeline',\n",
    "   description='A pipeline that trains on mnist dataset'\n",
    ")\n",
    "def mnist_pipeline():\n",
    "    log_folder = '/information'\n",
    "    pvc_name = \"mnist-model\"\n",
    "\n",
    "    image = 'tensorflow/tensorflow'\n",
    "    \n",
    "    # Creating volume\n",
    "    vop = dsl.VolumeOp(\n",
    "        name=pvc_name,\n",
    "        resource_name=\"mnist-model\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO\n",
    "    )\n",
    "\n",
    "    load_data_op = create_component_from_func(\n",
    "        load_data, \n",
    "        base_image=image\n",
    "        )\n",
    "    \n",
    "    split_data_op = create_component_from_func(\n",
    "        split_data, \n",
    "        base_image=image\n",
    "        )\n",
    "    \n",
    "    training_op = create_component_from_func(\n",
    "        create_train_model, \n",
    "        base_image=image\n",
    "        )\n",
    "\n",
    "\n",
    "    load_task = load_data_op(log_folder).add_pvolumes({ log_folder:vop.volume, })\n",
    "    split_data_task =split_data_op(load_task.outputs['datadir']).add_pvolumes({ log_folder:vop.volume, })\n",
    "    train_model_task  = training_op(split_data_task.outputs['data_folder']).add_pvolumes({ log_folder:vop.volume, })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c76c18-c77e-4ef8-85c6-48fbeda7166b",
   "metadata": {},
   "source": [
    "# Pipeline run\n",
    "- Here pipeline experiment name, function is defined first, then using the pipeline funcion and yaml filename the pipeline is compiled that generated the .yaml file. Then the pipeline is uploaded using the client and it is executed to run at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4104b877-76c1-4760-a74f-7e4530f4225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/758601b3-e456-4afe-8146-f46fbdaa5dc5\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/4716c348-d111-475f-b3bc-19d0d163ae27\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import uuid\n",
    "EXPERIMENT_NAME = \"mnist_pipeline\"\n",
    "pipeline_func = mnist_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + f'{uuid.uuid1()}.pipeline.yaml'\n",
    "kfp.compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "client = kfp.Client()\n",
    "experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "run_name = pipeline_func.__name__ + str(datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
    "client.upload_pipeline(pipeline_filename)\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5547e-e0ed-48f6-8991-e628630d3193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124ba1d-1a01-406b-ab67-0ea766cc3141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff0079-2cb9-4f08-928a-bc1799a8c160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bac9b3-5248-42d5-983e-7887ea73d91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36df0fb-bfc8-43e3-bcc0-ba76e4edc2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c013b48-8d4f-405e-9b5c-76c4b3d3d801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
