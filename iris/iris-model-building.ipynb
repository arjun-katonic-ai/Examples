{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Iris Dataset Description\n",
    "\n",
    "#### Dataset includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
    "\n",
    "#### The columns in this dataset are:\n",
    "\n",
    "- Id\n",
    "- SepalLengthCm\n",
    "- SepalWidthCm\n",
    "- PetalLengthCm\n",
    "- PetalWidthCm\n",
    "- Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from pyspark.sql import SparkSession\n",
    "from subprocess import run, Popen, PIPE\n",
    "from pyspark.sql import DataFrame, SparkSession, Window\n",
    "from pyspark.sql.functions import col, expr, monotonically_increasing_id, row_number,current_timestamp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import  List\n",
    "from datetime import datetime\n",
    "from minio import Minio\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import os\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "INCOME_MODEL_PATH=\"sklearn/iris/model\"\n",
    "EXPLAINER_MODEL_PATH=\"sklearn/iris/explainer\"\n",
    "OUTLIER_MODEL_PATH=\"sklearn/iris/outlier\"\n",
    "\n",
    "MINIO_HOST=\"minio-service.kubeflow:9000\"\n",
    "MINIO_ACCESS_KEY=\"minio\"\n",
    "MINIO_SECRET_KEY=\"minio123\"\n",
    "MINIO_MODEL_BUCKET=\"seldon\"\n",
    "\n",
    "DEPLOY_NAMESPACE=\"kubeflow\"\n",
    "\n",
    "EVENT_TIMESTAMP_ALIAS = \"event_timestamp\"\n",
    "CREATED_TIMESTAMP_ALIAS = \"created_timestamp\"\n",
    "\n",
    "STORAGEACCOUNTNAME= \"katonicusecases\"\n",
    "STORAGEACCOUNTKEY= \"kxGVhR3tKmJoNdEFbhauyHOvBaNMEJR8/uIH+4NKX9QLbHEsEhmo5YQmuiUmSaW2g/96Fq3RrV9f3FeMyizzgg==\"    \n",
    "CONTAINERNAME= \"modelbuilding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "def get_minio():\n",
    "    return Minio(MINIO_HOST,\n",
    "                    access_key=MINIO_ACCESS_KEY,\n",
    "                    secret_key=MINIO_SECRET_KEY,\n",
    "                    secure=False)\n",
    "\n",
    "def save_to_feature_store(spark,pandas_df,feaurestore_name,STORAGEACCOUNTNAME,CONTAINERNAME):\n",
    "\n",
    "    pandas_df['Unique_id']=np.random.choice(len(pandas_df), size=len(pandas_df), replace=False)\n",
    "    pandas_df['event_timestamp']=pd.to_datetime(datetime.now())\n",
    "    pandas_df['created_timestamp']=pd.to_datetime(datetime.now())\n",
    "    \n",
    "    df_var = spark.createDataFrame(pandas_df)\n",
    "    \n",
    "    op_path = \"stagging\" + \"/\" + feaurestore_name\n",
    "\n",
    "    op_path = \"wasbs://\" + CONTAINERNAME + \"@\" + STORAGEACCOUNTNAME + \".blob.core.windows.net/\"+op_path\n",
    "    \n",
    "    df_var.write.mode(\"overwrite\").parquet(op_path)\n",
    "    \n",
    "\n",
    "\n",
    "def create_entity_df(spark,unique_id,feaurestore_name,STORAGEACCOUNTNAME,CONTAINERNAME):\n",
    "    \n",
    "    \n",
    "    op_path = \"stagging\" + \"/\" + feaurestore_name\n",
    "\n",
    "    op_path = \"wasbs://\" + CONTAINERNAME + \"@\" + STORAGEACCOUNTNAME + \".blob.core.windows.net/\"+op_path\n",
    "    \n",
    "    df_var = spark.read.parquet(op_path)\n",
    "    \n",
    "    \n",
    "    entity_df= df_var.select(unique_id).withColumn('event_timestamp',current_timestamp())\n",
    "    \n",
    "    return entity_df\n",
    "\n",
    "def fetch_df(spark,feaurestore_name,STORAGEACCOUNTNAME,CONTAINERNAME):\n",
    "    \n",
    "    #os.mkdir('fs_logs')\n",
    "\n",
    "    op_path = \"stagging\" + \"/\" + feaurestore_name\n",
    "\n",
    "    op_path = \"wasbs://\" + CONTAINERNAME + \"@\" + STORAGEACCOUNTNAME + \".blob.core.windows.net/\"+op_path\n",
    "    \n",
    "    df_var = spark.read.parquet(op_path)\n",
    "    \n",
    "    #df_var = spark.createDataFrame(data)\n",
    "    \n",
    "    #shutil.rmtree('fs_logs')\n",
    "    return df_var\n",
    "\n",
    "def create_df_feature(spark,path_dict,feature_dict,STORAGEACCOUNTNAME,CONTAINERNAME):\n",
    "    df_list = []\n",
    "    f_list  = []\n",
    "\n",
    "    for fs,obj,features in zip(path_dict.keys(),path_dict.values(),feature_dict.values()):\n",
    "        #df = fetch_df(spark,fs,obj,STORAGEACCOUNTNAME,CONTAINERNAME)\n",
    "        df = fetch_df(spark,fs,STORAGEACCOUNTNAME,CONTAINERNAME)\n",
    "        df_list.append(df)\n",
    "        f_list.append(features)\n",
    "    return [df_list,f_list]\n",
    "\n",
    "def as_of_join(\n",
    "    entity_df: DataFrame,\n",
    "    feature_table_entity_names : list,\n",
    "    feature_table_df : DataFrame,\n",
    "    feature_list : list,\n",
    "    feature_table_name : str,\n",
    "    max_age = [],\n",
    "    entity_event_timestamp_column = 'event_timestamp'\n",
    "\n",
    "    ) -> DataFrame:\n",
    "    #print (feature_list)\n",
    "    #print(type(feature_list))\n",
    "    feature_table_df = feature_table_df.select(feature_list+[EVENT_TIMESTAMP_ALIAS,CREATED_TIMESTAMP_ALIAS,feature_table_entity_names[0]])\n",
    "    entity_with_id = entity_df.withColumn(\"_row_nr\", monotonically_increasing_id())\n",
    "    feature_event_timestamp_column_with_prefix = (\n",
    "        f\"{feature_table_name}__{EVENT_TIMESTAMP_ALIAS}\"\n",
    "        )\n",
    "    feature_created_timestamp_column_with_prefix = (\n",
    "        f\"{feature_table_name}__{CREATED_TIMESTAMP_ALIAS}\"\n",
    "        )\n",
    "\n",
    "    projection = [\n",
    "        col(col_name).alias(f\"{feature_table_name}__{col_name}\")\n",
    "        for col_name in feature_table_df.columns\n",
    "        ]\n",
    "\n",
    "    aliased_feature_table_df = feature_table_df.select(projection)\n",
    "    \n",
    "    join_cond = (\n",
    "    entity_with_id[entity_event_timestamp_column]\n",
    "        >= aliased_feature_table_df[feature_event_timestamp_column_with_prefix]\n",
    "    )\n",
    "    if max_age:\n",
    "        join_cond = join_cond & (\n",
    "        aliased_feature_table_df[feature_event_timestamp_column_with_prefix]\n",
    "        >= entity_with_id[entity_event_timestamp_column]\n",
    "        - expr(f\"INTERVAL {max_age[0]} seconds\")\n",
    "        )\n",
    "    for key in feature_table_entity_names:\n",
    "        join_cond = join_cond & (\n",
    "        entity_with_id[key]\n",
    "        == aliased_feature_table_df[f\"{feature_table_name}__{key}\"]\n",
    "        )\n",
    "    conditional_join = entity_with_id.join(\n",
    "        aliased_feature_table_df, join_cond, \"leftOuter\"\n",
    "        )\n",
    "    for key in feature_table_entity_names:\n",
    "        conditional_join = conditional_join.drop(\n",
    "        aliased_feature_table_df[f\"{feature_table_name}__{key}\"]\n",
    "        )\n",
    "    window = Window.partitionBy(\"_row_nr\", *feature_table_entity_names).orderBy(\n",
    "        col(feature_event_timestamp_column_with_prefix).desc(),\n",
    "        col(feature_created_timestamp_column_with_prefix).desc(),\n",
    "        )\n",
    "    filter_most_recent_feature_timestamp = conditional_join.withColumn(\n",
    "        \"_rank\", row_number().over(window)\n",
    "        ).filter(col(\"_rank\") == 1)\n",
    "    return filter_most_recent_feature_timestamp.select(\n",
    "        entity_df.columns\n",
    "        + [\n",
    "            f\"{feature_table_name}__{feature}\"\n",
    "            for feature in feature_list\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "def retrieve_feature(\n",
    "    entity_df: DataFrame,\n",
    "    feature_table_dfs:List[DataFrame],\n",
    "    feature_lists :List[list],\n",
    "    feature_table_names:list,\n",
    "    feature_table_entity_names : List[str],\n",
    "    max_age=[],\n",
    "    entity_event_timestamp_column='event_timestamp',\n",
    "    ) -> DataFrame :\n",
    "    \n",
    "    joined_df = entity_df\n",
    "\n",
    "    for (feature_table_df, feature_list,feature_table_name) in zip(feature_table_dfs, feature_lists,feature_table_names ):\n",
    "            joined_df = as_of_join(\n",
    "                joined_df, feature_table_entity_names,feature_table_df, feature_list,feature_table_name,\n",
    "            max_age = max_age,\n",
    "        entity_event_timestamp_column = entity_event_timestamp_column)\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "block:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-azure:2.7.3,com.microsoft.azure:azure-storage:2.2.0,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'\n",
    "spark =  SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "spark.conf.set(\"fs.wasbs.impl\",\"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "spark.conf.set('fs.azure.account.key.' + STORAGEACCOUNTNAME + '.blob.core.windows.net', STORAGEACCOUNTKEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris= fetch_df(spark,'iris',STORAGEACCOUNTNAME,CONTAINERNAME).toPandas().drop(['Unique_id', 'event_timestamp','created_timestamp'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0            5.1           3.5            1.4           0.2        0\n",
       "1            4.9           3.0            1.4           0.2        0\n",
       "2            4.7           3.2            1.3           0.2        0\n",
       "3            4.6           3.1            1.5           0.2        0\n",
       "4            5.0           3.6            1.4           0.2        0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- train = 70\n",
    "- test = 30\n",
    "\n",
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "block:logisticregression",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=110)\n",
    "lr = logreg.fit(X_train_lr, y_train_lr)\n",
    "acc_log = round(logreg.score(X_train_lr, y_train_lr) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "block:randomforest",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "rf = random_forest.fit(X_train_rf, y_train_rf)\n",
    "acc_random_forest = round(random_forest.score(X_train_rf, y_train_rf) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "block:gaussiannaivebayes",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian = GaussianNB()\n",
    "gnb = gaussian.fit(X_train_gnb, y_train_gnb)\n",
    "acc_gaussian = round(gaussian.score(X_train_gnb, y_train_gnb) * 100, 2)\n",
    "acc_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "block:supportvectormachine",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.05"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svc = SVC(gamma='auto')\n",
    "svm = linear_svc.fit(X_train_svm, y_train_svm)\n",
    "acc_linear_svc = round(linear_svc.score(X_train_svm, y_train_svm) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "block:decisiontree",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "dt = decision_tree.fit(X_train_dt, y_train_dt)\n",
    "acc_decision_tree = round(decision_tree.score(X_train_dt, y_train_dt) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Preceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "block:preceptron",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_per, X_test_per, y_train_per, y_test_per = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.95"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "per = perceptron.fit(X_train_per, y_train_per)\n",
    "acc_perceptron = round(perceptron.score(X_train_per, y_train_per) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### K Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "block:knearestneighbour",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.14"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knearestneighbour = KNeighborsClassifier()\n",
    "knn = knearestneighbour.fit(X_train_knn, y_train_knn)\n",
    "acc_knearest = round(knearestneighbour.score(X_train_knn, y_train_knn) * 100, 2)\n",
    "acc_knearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "block:stochasticgradientdescent",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_sgd, X_test_sgd, y_train_sgd, y_test_sgd = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.71"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochasticgrad = SGDClassifier()\n",
    "sgd = stochasticgrad.fit(X_train_sgd, y_train_sgd)\n",
    "acc_sgd = round(stochasticgrad.score(X_train_sgd, y_train_sgd) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "block:gradientboostingclassifier",
     "prev:feature_store"
    ]
   },
   "outputs": [],
   "source": [
    "X = iris.drop(columns = ['Species'])\n",
    "Y = iris['Species']\n",
    "X_train_gbc, X_test_gbc, y_train_gbc, y_test_gbc = train_test_split(X, Y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientboostingclassifier = GradientBoostingClassifier()\n",
    "gbc = gradientboostingclassifier.fit(X_train_gbc, y_train_gbc)\n",
    "acc_gbc = round(gradientboostingclassifier.score(X_train_gbc, y_train_gbc) * 100, 2)\n",
    "acc_gbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "block:model_evaluation",
     "prev:logisticregression",
     "prev:randomforest",
     "prev:gaussiannaivebayes",
     "prev:supportvectormachine",
     "prev:decisiontree",
     "prev:preceptron",
     "prev:knearestneighbour",
     "prev:stochasticgradientdescent",
     "prev:gradientboostingclassifier"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Model  \\\n",
      "Score                                  \n",
      "100.00                 Random Forest   \n",
      "100.00                 Decision Tree   \n",
      "100.00  Gradient Boosting Classifier   \n",
      "99.05         Support Vector Machine   \n",
      "98.10            Logistic Regression   \n",
      "97.14             KNearest Neighbour   \n",
      "94.29           Gaussian Naive Bayes   \n",
      "80.95                     Preceptron   \n",
      "65.71    Stochastic Gradient Descent   \n",
      "\n",
      "                                                Model_abb  \n",
      "Score                                                      \n",
      "100.00  (DecisionTreeClassifier(max_features='auto', r...  \n",
      "100.00                           DecisionTreeClassifier()  \n",
      "100.00  ([DecisionTreeRegressor(criterion='friedman_ms...  \n",
      "99.05                                   SVC(gamma='auto')  \n",
      "98.10                    LogisticRegression(max_iter=110)  \n",
      "97.14                              KNeighborsClassifier()  \n",
      "94.29                                        GaussianNB()  \n",
      "80.95                                        Perceptron()  \n",
      "65.71                                     SGDClassifier()  \n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'Gaussian Naive Bayes', 'Support Vector Machine', 'Decision Tree', 'Preceptron', 'KNearest Neighbour', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n",
    "    'Score': [acc_log, acc_random_forest, acc_gaussian, acc_linear_svc, acc_decision_tree, acc_perceptron, acc_knearest, acc_sgd, acc_gbc],\n",
    "    'Model_abb': [lr, rf, gnb, svm, dt, per, knn, sgd, gbc]})\n",
    "result_df = results.sort_values(by='Score', ascending=False)\n",
    "result_df = result_df.set_index('Score')\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = result_df['Model_abb'].iloc[0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(best_model, 'model.joblib')\n",
    "# print(get_minio().fput_object(MINIO_MODEL_BUCKET, f\"{INCOME_MODEL_PATH}/model.joblib\", 'model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_job = joblib.load(\"model.joblib\")\n",
    "Model_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.ensemble._forest.RandomForestClassifier"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Model_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = X_test_rf.values[44].reshape(1, -1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_job.predict(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<minio.helpers.ObjectWriteResult object at 0x7fa732cf7550>\n"
     ]
    }
   ],
   "source": [
    "print(get_minio().fput_object(MINIO_MODEL_BUCKET, f\"{INCOME_MODEL_PATH}/model.joblib\", 'model.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": false,
   "docker_image": "katonic/usecase:2.0",
   "experiment": {
    "id": "new",
    "name": "customer-churn"
   },
   "experiment_name": "customer-churn",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "Predict behavior to retain customers",
   "pipeline_name": "customer-churn-1",
   "snapshot_volumes": false,
   "steps_defaults": [],
   "volume_access_mode": "rwo",
   "volumes": []
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
